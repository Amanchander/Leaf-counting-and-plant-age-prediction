# -*- coding: utf-8 -*-
"""Copy of old cafe.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LoMF1xI-TBik4zgGc9_i3ariaL5Vx1Gg
"""


import tensorflow as tf
import numpy as np
import math

import pickle
from matplotlib import pyplot as plt
from sklearn.model_selection import train_test_split

from tensorflow.keras import layers
import tensorflow_addons as tfa



np.random.seed(42)
tf.random.set_seed(10)  

def load_pickle_data(pickle_file):
    with open(pickle_file, "rb") as f:
        image_filenames, image_array, metadata_list = pickle.load(f)
    print(f"Data loaded from {pickle_file}")

    # Sort metadata and corresponding images based on (crop, plant, day, level, angle)
    sorted_indices = sorted(
        range(len(metadata_list)),
        key=lambda i: (
            metadata_list[i]["crop"], 
            metadata_list[i]["plant"], 
            metadata_list[i]["day"], 
            metadata_list[i]["level"], 
            metadata_list[i]["angle"],
            metadata_list[i]["leaf_count"],            
        )
    )

    # Apply sorting to all lists
    image_filenames = [image_filenames[i] for i in sorted_indices]
    image_array = np.array([image_array[i] for i in sorted_indices], dtype=object)
    metadata_list = [metadata_list[i] for i in sorted_indices]

    return image_filenames, image_array, metadata_list



# Example usage
pickle_file_path = "F:/Aman/Academics work/Awadh work/Grand challenge/ACM grand challenge/Code/Pkl files/leaf_counting_data.pkl"
image_name, image_arr, meta_data  = load_pickle_data(pickle_file_path)

 

# Print ground truths
for i, data in enumerate(meta_data):
    print(f"Crop: {data['crop']}, Img_count: {i+1}, Age: {data['day']}, Leaf Count: {data['leaf_count']}")



levels = [1, 2, 3, 4, 5]
no_of_input_images = 24
starting_angle = 0
 

def input_angles(no_of_input_images):
    idx_diff = int(24/no_of_input_images)
    selected_angles = []
    
    val = starting_angle
    
    for i in range(0, no_of_input_images): 
        selected_angles.append(val)
        val = val + (idx_diff * 15)
        
        if val>345:
            val = abs(val - 360)
        
    return selected_angles


required_angles = input_angles(no_of_input_images)
print(required_angles)



def get_required_images(images_file_names, image_arr, meta_data, req_angles, level_array):
    req_image_filename = []
    req_image_arr = []
    req_met_data = []
    
    for (image_name, img_arr, met_data) in zip(images_file_names, image_arr, meta_data):

        if (met_data['angle'] in req_angles) and (met_data['level'] in level_array):
            print("Angle is ", met_data['angle'], " and level is ", met_data['level']) 
            req_image_filename.append(image_name)
            req_image_arr.append(img_arr)
            req_met_data.append(met_data)
            
            
    return (req_image_filename, req_image_arr, req_met_data)
    


(images_name, images_array, meta_data) = get_required_images(image_name, image_arr, meta_data, required_angles, levels)

# print(images_name)
# print(images_array)
# print(meta_data)


def get_required_images_for_same_day(img_name, img_array, met_data):
    curr_day = met_data[0]['day']  # Initialize with the first image's day
    imgs = []
    days = []
    leaf_count = []
    
    temp_images = []  # Temporary list to store images of the same day
    
    try:
        
        for i, (name, img, data) in enumerate(zip(img_name, img_array, met_data)):
            if data['day'] == curr_day: 
                temp_images.append(img)
                print(f"Curr day is {data['day']}, Processing: {name}")
                
            else:
                print(f"Switching to another day: {data['day']}")
    
                # Store metadata before moving to the next day
                days.append(curr_day)  
                leaf_count.append(met_data[i - 1]['leaf_count'])  # Store the last image's leaf count
                
                if temp_images:
                    try:
                        stacked_image = np.concatenate(temp_images, axis=2)  # Concatenating along depth
                                                
                        expected_channel_size = len(levels) * no_of_input_images * 3
                                               
                        if np.array(stacked_image).shape[-1] < expected_channel_size:
                            
                            diff = expected_channel_size - np.array(stacked_image).shape[-1]
                            
                            for repeat in range(0, int(diff/3)):
                                stacked_image = np.concatenate((stacked_image, img), axis=2)       
                        
                        imgs.append(stacked_image)
                        
                        
                    except ValueError as e:
                        print(f"Skipping concatenation for day {curr_day} due to shape mismatch: {e}")
                        imgs.append(temp_images[0])  # Save at least one image
    
                temp_images = [img]  # Start new list for the next day
                curr_day = data['day']
                
                print("Stacked image size in else is ", np.array(stacked_image).shape)
    
    
        # Handle the last batch
        if temp_images:
            try:
                stacked_image = np.concatenate(temp_images, axis=2)
                imgs.append(stacked_image)
            except ValueError as e:
                print(f"Skipping concatenation for last batch due to shape mismatch: {e}")
                imgs.append(temp_images[0])
    
            # Append metadata for the last batch
            days.append(curr_day)
            leaf_count.append(met_data[-1]['leaf_count'])
        
        imgs = np.array(imgs)
        
        return np.array(imgs, dtype=object), days, leaf_count 

    except Exception as e:
        print(e)



# Example Usage:
images, days, leaf_counts = get_required_images_for_same_day(images_name, images_array, meta_data)
print("The output size is ", images.shape)
print("Days size is ", len(days))
print("Leaf counts size is ", len(leaf_counts))


# Define the custom generator
class My_Custom_Generator(tf.keras.utils.Sequence):
    def __init__(self, images, days, leaf_counts, batch_size):
        """
        Custom Data Generator for training.

        :param images: List/Array of processed images.
        :param days: Ground truth day values.
        :param leaf_counts: Ground truth leaf count values.
        :param batch_size: Number of samples per batch.
        """
        self.images = np.array(images, dtype=np.float32)
        self.days = np.array(days, dtype=np.float32)
        self.leaf_counts = np.array(leaf_counts, dtype=np.float32)
        self.batch_size = batch_size
        self.indices = np.arange(len(self.images))

    def __len__(self):
        """Returns the total number of batches per epoch."""
        return int(np.floor(len(self.images) / self.batch_size))

    def __getitem__(self, idx):
        """Generates a batch of data."""
        batch_indices = self.indices[idx * self.batch_size : (idx+1) * self.batch_size]
        batch_images = self.images[batch_indices]
        batch_days = self.days[batch_indices]
        batch_leaf_counts = self.leaf_counts[batch_indices]

        # Normalize image channels
        batch_images[..., :3] /= 255.0  # Normalize RGB channels
        batch_images[..., 3] /= np.max(batch_images[..., 3]) if np.max(batch_images[..., 3]) > 0 else 1

        return batch_images, {"day": batch_days, "leaf_count": batch_leaf_counts}

    def on_epoch_end(self):
        """Shuffles data at the end of each epoch."""
        np.random.shuffle(self.indices)
        

# Define indices for training and validation sets
split_index = int(len(images) * 0.8)  # 80% for training, 20% for validation

# Manually split the dataset
train_x = images[:split_index] 
valid_x = images[split_index:]

train_days = days[:split_index]
valid_days = days[split_index:]

train_leaf_counts = leaf_counts[:split_index]
valid_leaf_counts = leaf_counts[split_index:]


# Define batch size
batch_size = 2

# Create training and validation generators
train_generator = My_Custom_Generator(train_x, train_days, train_leaf_counts, batch_size)
valid_generator = My_Custom_Generator(valid_x, valid_days, valid_leaf_counts, batch_size)

# Define the Vision Transformer model with dual outputs
def build_vit_model(input_shape, patch_size, num_patches, projection_dim, num_heads, mlp_dim, dropout_rate):
    inputs = layers.Input(shape=input_shape)

    # Extract patches
    patches = layers.Reshape((num_patches, patch_size * patch_size * input_shape[-1]))(
        layers.Lambda(lambda x: tf.image.extract_patches(
            x, sizes=[1, patch_size, patch_size, 1],
            strides=[1, patch_size, patch_size, 1],
            rates=[1, 1, 1, 1], padding='VALID'
        ))(inputs)
    )

    # Separate channels
    rgb_channels = layers.Lambda(lambda x: x[:, :, :patch_size * patch_size * 3])(patches)
    alpha_channel = layers.Lambda(lambda x: x[:, :, patch_size * patch_size * 3:])(patches)

    # Projection layers for RGB and Alpha channels
    projection_layer_rgb = layers.Dense(units=int(projection_dim/2), activation='linear')
    projection_layer_alpha = layers.Dense(units=int(projection_dim/2), activation='linear')

    x_rgb = projection_layer_rgb(rgb_channels)
    x_alpha = projection_layer_alpha(alpha_channel)

    # Add position embeddings
    position_embedding_layer = layers.Embedding(input_dim=num_patches, output_dim=int(projection_dim/2))
    positions = tf.range(start=0, limit=num_patches, delta=1)
    position_embeddings = position_embedding_layer(positions)

    x_rgb += position_embeddings
    x_alpha += position_embeddings

    # Concatenate RGB and Alpha channels
    x = layers.Concatenate(axis=-1)([x_rgb, x_alpha])

    # Transformer Encoder layers
    for _ in range(num_heads):
        attention_output = tfa.layers.MultiHeadAttention(num_heads=num_heads, head_size=projection_dim // num_heads)([x, x])
        attention_output = layers.Dropout(dropout_rate)(attention_output)
        attention_output = layers.LayerNormalization(epsilon=1e-6)(attention_output + x)

        # Feed Forward Network
        ffn = layers.Dense(units=mlp_dim, activation='relu')(attention_output)
        ffn = layers.Dense(units=projection_dim, activation='linear')(ffn)
        ffn = layers.Dropout(dropout_rate)(ffn)
        x = layers.LayerNormalization(epsilon=1e-6)(ffn + attention_output)

    # Flatten output
    x = layers.Flatten()(x)

    # Dual output layers: one for "day" and one for "leaf_count"
    day_output = layers.Dense(units=1, activation='linear', name="day")(x)
    leaf_count_output = layers.Dense(units=1, activation='linear', name="leaf_count")(x)

    # Create the model
    model = tf.keras.models.Model(inputs=inputs, outputs=[day_output, leaf_count_output])

    return model

# Define model parameters
PATCH_SIZE = 4
NUM_PATCHES = (128 // PATCH_SIZE) * (128 // PATCH_SIZE)

# Build the model
model = build_vit_model(input_shape=(128, 128, len(levels) * no_of_input_images * 3),
                        patch_size=PATCH_SIZE,
                        num_patches=NUM_PATCHES,
                        projection_dim=64,
                        num_heads=4,
                        mlp_dim=32,
                        dropout_rate=0.2)

# Compile the model
learning_rate = 0.001
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
              loss={"day": tf.keras.losses.MeanSquaredError(),
                    "leaf_count": tf.keras.losses.MeanSquaredError()},
              metrics={"day": tf.keras.losses.MeanSquaredLogarithmicError(),
                       "leaf_count": tf.keras.losses.MeanSquaredLogarithmicError()})

# Model training
history = model.fit(train_generator,
                    epochs=25,
                    verbose=1,
                    validation_data=valid_generator) 

# Save the trained model
model.save('saved_model/vit_dual_output.h5')

# Plot training loss
import matplotlib.pyplot as plt

plt.plot(history.history['day_loss'])
plt.plot(history.history['val_day_loss'])
plt.plot(history.history['leaf_count_loss'])
plt.plot(history.history['val_leaf_count_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train Day', 'Val Day', 'Train Leaf Count', 'Val Leaf Count'], loc='upper left')
plt.show()



y_pred_days = []
y_pred_leaf_counts = []

for img in valid_x:
    img[..., :3] /= 255.0
    img[..., 3] /= np.amax(img[..., 3]) if np.amax(img[..., 3]) > 0 else 1
    img = np.expand_dims(img, axis=0).astype(np.float32)  # Ensure correct dtype

    pred_output = model.predict(img, verbose=0)

    if isinstance(pred_output, (list, tuple)):  # If model has two outputs
        pred_day, pred_leaf = pred_output
        y_pred_days.append(pred_day[0, 0])
        y_pred_leaf_counts.append(pred_leaf[0, 0])
    else:  # If model has a single output with two values
        y_pred_days.append(pred_output[0, 0])
        y_pred_leaf_counts.append(pred_output[0, 1])

# Convert to NumPy arrays
y_true_days = np.array(valid_days, dtype=np.float32)
y_true_leaf_counts = np.array(valid_leaf_counts, dtype=np.float32)
y_pred_days = np.array(y_pred_days, dtype=np.float32)
y_pred_leaf_counts = np.array(y_pred_leaf_counts, dtype=np.float32)


import tensorflow as tf

# Compute RMSE
rmse_days = tf.sqrt(tf.keras.metrics.mean_squared_error(y_true_days, y_pred_days))
rmse_leaf_counts = tf.sqrt(tf.keras.metrics.mean_squared_error(y_true_leaf_counts, y_pred_leaf_counts))

# Compute MAE
mae_days = tf.keras.metrics.mean_absolute_error(y_true_days, y_pred_days)
mae_leaf_counts = tf.keras.metrics.mean_absolute_error(y_true_leaf_counts, y_pred_leaf_counts)

# Print results
print("RMSE (Days):", rmse_days.numpy())
print("RMSE (Leaf Counts):", rmse_leaf_counts.numpy())
print("MAE (Days):", mae_days.numpy())
print("MAE (Leaf Counts):", mae_leaf_counts.numpy())
